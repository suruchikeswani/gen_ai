{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGmJSIHZKBhxUw3vN/8fCc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install git+https://github.com/openai/swarm.git"],"metadata":{"id":"ZZ8SnJluMBiX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install openai firecrawl-py serpapi google-search-results"],"metadata":{"id":"83J1LSWPLVKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfobA_VRI9RA","executionInfo":{"status":"ok","timestamp":1729583972145,"user_tz":-330,"elapsed":455,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}},"outputId":"bd2bd5f9-4707-4dd6-d5d2-12fd1ff34dec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main.py\n"]}],"source":["%%writefile main.py\n","\n","# Importing the necessary libraries\n","import os\n","from firecrawl import FirecrawlApp\n","from swarm import Agent\n","from swarm.repl import run_demo_loop\n","import dotenv\n","from serpapi import GoogleSearch\n","from openai import OpenAI\n","\n","# Load the env having API Keys - OpenAI, FireCrawl and SerpAPI\n","dotenv.load_dotenv()\n","\n","# Initialize FirecrawlApp and OpenAI\n","app = FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))\n","client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n","\n","# Defining functions for agents\n","def search_google(query, objective):\n","    \"\"\"Search Google using SerpAPI.\"\"\"\n","    print(f\"Parameters: query={query}, objective={objective}\")\n","    search = GoogleSearch({\"q\": query, \"api_key\": os.getenv(\"SERP_API_KEY\")})\n","    results = search.get_dict().get(\"organic_results\", [])\n","    return {\"objective\": objective, \"results\": results}\n","\n","def map_url_pages(url, objective):\n","    \"\"\"Map a website's pages using Firecrawl.\"\"\"\n","    search_query = generate_completion(\n","        \"website search query generator\",\n","        f\"Generate a 1-2 word search query for the website: {url} based on the objective\",\n","        \"Objective: \" + objective\n","    )\n","    print(f\"Parameters: url={url}, objective={objective}, search_query={search_query}\")\n","    map_status = app.map_url(url, params={'search': search_query})\n","    if map_status.get('status') == 'success':\n","        links = map_status.get('links', [])\n","        top_link = links[0] if links else None\n","        return {\"objective\": objective, \"results\": [top_link] if top_link else []}\n","    else:\n","        return {\"objective\": objective, \"results\": []}\n","\n","def scrape_url(url, objective):\n","    \"\"\"Scrape a website using Firecrawl.\"\"\"\n","    print(f\"Parameters: url={url}, objective={objective}\")\n","    scrape_status = app.scrape_url(\n","        url,\n","        params={'formats': ['markdown']}\n","    )\n","    return {\"objective\": objective, \"results\": scrape_status}\n","\n","def analyze_website_content(content, objective):\n","    \"\"\"Analyze the scraped website content using OpenAI.\"\"\"\n","    print(f\"Parameters: content={content[:50]}..., objective={objective}\")\n","    analysis = generate_completion(\n","        \"website data extractor\",\n","        f\"Analyze the following website content and extract a JSON object based on the objective.\",\n","        \"Objective: \" + objective + \"\\nContent: \" + content\n","    )\n","    return {\"objective\": objective, \"results\": analysis}\n","\n","def generate_completion(role, task, content):\n","    \"\"\"Generate a completion using OpenAI.\"\"\"\n","    print(f\"Parameters: role={role}, task={task[:50]}..., content={content[:50]}...\")\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": f\"You are a {role}. {task}\"},\n","            {\"role\": \"user\", \"content\": content}\n","        ]\n","    )\n","    return response.choices[0].message.content\n","\n","# Defining handoffs for context variable updations\n","def handoff_to_search_google():\n","    \"\"\"Hand off the search query to the search google agent.\"\"\"\n","    return google_search_agent\n","\n","def handoff_to_map_url():\n","    \"\"\"Hand off the url to the map url agent.\"\"\"\n","    return map_url_agent\n","\n","def handoff_to_website_scraper():\n","    \"\"\"Hand off the url to the website scraper agent.\"\"\"\n","    return website_scraper_agent\n","\n","def handoff_to_analyst():\n","    \"\"\"Hand off the website content to the analyst agent.\"\"\"\n","    return analyst_agent\n","\n","# Defining Agents\n","# UI agent for user-interaction\n","user_interface_agent = Agent(\n","    name=\"User Interface Agent\",\n","    instructions=\"You are a user interface agent that handles all interactions with the user. You need to always start with an web data extraction objective that the user wants to achieve by searching the web, mapping the web pages, and extracting the content from a specific page. Be concise.\",\n","    functions=[handoff_to_search_google],\n",")\n","\n","# Google search agent for searching web\n","google_search_agent = Agent(\n","    name=\"Google Search Agent\",\n","    instructions=\"You are a google search agent specialized in searching the web. Only search for the website not any specific page. When you are done, you must hand off to the map agent.\",\n","    functions=[search_google, handoff_to_map_url],\n",")\n","\n","# URL mapping agent for mapping web pages\n","map_url_agent = Agent(\n","    name=\"Map URL Agent\",\n","    instructions=\"You are a map url agent specialized in mapping the web pages. When you are done, you must hand off the results to the website scraper agent.\",\n","    functions=[map_url_pages, handoff_to_website_scraper],\n",")\n","\n","# Website scraper agent for scraping data off the website\n","website_scraper_agent = Agent(\n","    name=\"Website Scraper Agent\",\n","    instructions=\"You are a website scraper agent specialized in scraping website content. When you are done, you must hand off the website content to the analyst agent to extract the data based on the objective.\",\n","    functions=[scrape_url, handoff_to_analyst],\n",")\n","\n","# Analyst agent for understanding the website content and displaying in JSON format\n","analyst_agent = Agent(\n","    name=\"Analyst Agent\",\n","    instructions=\"You are an analyst agent that examines website content and returns a JSON object. When you are done, you must return a JSON object.\",\n","    functions=[analyze_website_content],\n",")\n","\n","if __name__ == \"__main__\":\n","    run_demo_loop(user_interface_agent, stream=True)"]},{"cell_type":"code","source":["!python main.py"],"metadata":{"id":"R2eDDMtxKv1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Qn-Ty59YMz3_"},"execution_count":null,"outputs":[]}]}